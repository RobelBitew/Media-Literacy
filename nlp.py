import os
import string
import nltk
nltk.download('stopwords')   # Uncomment if needed
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import normalize
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

alpha = 2.0
beta = 2.5
gamma = 2.5
delta = 1.8

# my word lists were generated by ChatGPT - I acknowledge the potential for false positives/negatives
# also, "democratic" words can be co-opted by authoritarian leaders
imperative_words = [
    "must", "obey", "command", "submit", "comply", "follow", "enforce", "require",
    "ban", "prohibit", "order", "dictate", "control", "demand", "forbid",
    "rule", "govern", "authorize", "restrict", "discipline", "suppress",
    "punish", "conform", "implement", "insist", "mandate", "commanding",
    "regulate", "permit", "dictatorship", "instruct", "direct", "administer",
    "impose", "supervise", "monitor", "decide", "assign", "obey", "submit", "punish", 
    "eliminate", "control", "suppress", "forbid", "execute", "ban", "silence", "dominate", 
    "enforce", "compel", "dictate", "restrict", "exterminate", "order", "authority", "command", 
    "regime", "law", "punishment", "obedience", "discipline", "decree", "army", "loyalty", "unity", 
    "sacrifice", "traitor", "enemy", "danger", "threat", "crisis", "chaos", "collapse", 
    "destroy", "terrorist", "infidel", "invade", "overrun", "plague", "betray", 
    "shame", "sin", "disgrace", "ungrateful", "degenerate", "immoral", "corrupt", 
    "wicked", "decay", "filth", "always", "never", "only", "must", "everyone", 
    "no one", "forever", "inevitable", "vermin", "disease", "parasite", 
    "infestation", "criminal", "subhuman", "illegals", "patriot", "true citizen", 
    "faithful", "allegiance", "loyalty", "devotion", "glory", "submission", "unite", 
    "control", "silence", "ban", "monitor", "censor", "cleanse", "purify", "eradicate", "re-educate",
    "glorious", "destiny", "greatness", "salvation", "rescue", "rebirth", "reclaim", 
    "purge", "heritage", "betrayal", "decay", "moral collapse", "patriotism", "righteous", 
    "sacrifice", "martyr", "traitor", "enemy within", "infestation"

]

democratic_words = [
    "freedom", "liberty", "justice", "equality", "rights", "vote", "election", "consent",
    "participation", "accountability", "transparency", "representative", "democracy",
    "civil", "dialogue", "open", "inclusive", "voice", "respect", "tolerance", "pluralism",
    "debate", "lawful", "constitutional", "checks", "balance", "community", "deliberation",
    "governance", "human", "dignity", "press", "independent", "assembly", "petition",
    "minority", "majority", "protest", "nonviolent", "equal", "shared", "cooperation", 
    "self-rule", "public", "rights-respecting", "rule of law", "civic", "collaborative", 
    "representation", "elect", "runoff", "referendum", "ballot", "open society", 
    "civil society", "coalition", "franchise"
]


def count_imperative_words(text, imperative_words):
    count = 0
    words = text.split()
    words_len = len(words)

    for word in words:
        if word in imperative_words:
            count+=1
    if words_len > 0:
        ratio = count / words_len
    else:
        ratio = 0
    return count, ratio

def count_democratic_words(text, democratic_words):
    count = 0
    words = text.split()
    words_len = len(words)

    for word in words:
        if word in democratic_words:
            count+=1
    if words_len > 0:
        ratio = count / words_len
    else:
        ratio = 0
    return count, ratio


def combine_cos_sim_and_word_count(sim_auth, sim_control, imperative_ratio, democratic_ratio):
    
    combined_auth = sim_auth + alpha * imperative_ratio - beta * democratic_ratio
    # these are the combined percentages... we have 4 parameters we are able to change: {alpha, beta, gamma, delta}
    # I wasn't able to fine tune these parameters but theoretically we could
    # we are taking the simlarity averages and combining that with the word count ratios
    combined_ctrl = sim_control + gamma * democratic_ratio - delta * imperative_ratio
    
    return {
        "combined_auth_score": combined_auth,
        "combined_ctrl_score": combined_ctrl
    }

def preprocess_texts(folder):
    data = []
    labels = []
    filenames = []
    stop_words = set(stopwords.words('english'))
    for label in ['authoritarian', 'control']:
        folder_path = os.path.join(folder, label)
        for fname in os.listdir(folder_path):
            with open(os.path.join(folder_path, fname), 'r', encoding='utf-8') as f:
                text = f.read().lower()
                # removes all punctuation & stopwords:
                text = text.translate(str.maketrans('', '', string.punctuation))
                individual_words = text.split()
                filtered_words = []
                for word in individual_words:
                    if word not in stop_words:
                        filtered_words.append(word)
                text = ' '.join(filtered_words)
                data.append(text)
                labels.append(label)
                filenames.append(fname)
    return data, labels, filenames



def feature_extraction(texts, labels, filenames):
    vectorizer = TfidfVectorizer(max_features=1000) # changed from count vectorizer
    #vectorizer = CountVectorizer(max_features=2000) # initializes vectorizer
    # we can choose 500 to 2,000 max features to avoid overfitting with only |corpus| = 11
    X = vectorizer.fit_transform(texts) # creates bag-of-(most common)-words
    # ^ this picks the 500 most common words across the entire dataset
    # word & word count
    # X ends up being a matrix = {document, word, count}
    X_normalized = normalize(X, norm='l2') 
    # normalizes using Euclidean norm, which is commonly needed for cosine similarity
    # the normalization makes it so that short speeches and long texts would have ratio-based word counts
    labels_array = np.array(labels) # converts to numpy array
    authoritarian_mean = X_normalized[labels_array == 'authoritarian'].mean(axis=0) 
    # computes average value across authoritarian documents
    control_mean = X_normalized[labels_array == 'control'].mean(axis=0)
    authoritarian_mean = authoritarian_mean.A # makes .toarray()
    control_mean = control_mean.A
    authoritarian_sim = cosine_similarity(X_normalized, authoritarian_mean)
    # cosine similarity is the angle or dot product between two vectors
    control_sim = cosine_similarity(X_normalized, control_mean)

    # now that we have cosine similarity, let's combine with word counts:
    for i in range(len(texts)):
        imperative_count, imperative_ratio = count_imperative_words(texts[i], imperative_words)
        democratic_count, democratic_ratio = count_democratic_words(texts[i], democratic_words)

        combined = combine_cos_sim_and_word_count(
            sim_auth = authoritarian_sim[i][0],
            sim_control = control_sim[i][0],
            imperative_ratio = imperative_ratio,
            democratic_ratio = democratic_ratio
        )

        # used ChatGPT to help me format these print statements:
        """ print(f"{labels[i]} | AuthSim: {authoritarian_sim[i][0]:.2f} | CtrlSim: {control_sim[i][0]:.2f} | "
            f"Imp: {imperative_count} ({imperative_ratio:.2%}) | Demo: {democratic_count} ({democratic_ratio:.2%}) | "
            f"CombAuth: {combined['combined_auth_score']:.2f} | CombCtrl: {combined['combined_ctrl_score']:.2f} | {filenames[i]}") """

    
    
    return vectorizer, authoritarian_mean, control_mean



def predict_similarity(vectorizer, authoritarian_mean, control_mean, text):
    from nltk.corpus import stopwords
    stop_words = set(stopwords.words('english'))
    text = text.lower()
    # removes all punctuation:
    text = text.translate(str.maketrans('', '', string.punctuation))
    individual_words = text.split()
    filtered_words = []
    for word in individual_words:
        if word not in stop_words:
            filtered_words.append(word)
    text = ' '.join(filtered_words)

    X_new = vectorizer.transform([text])
    X_new_normalized = normalize(X_new, norm='l2')

    authoritarian_sim = cosine_similarity(X_new_normalized, authoritarian_mean)[0][0]
    control_sim = cosine_similarity(X_new_normalized, control_mean)[0][0]

    imperative_count, imperative_ratio = count_imperative_words(text, imperative_words)
    democratic_count, democratic_ratio = count_democratic_words(text, democratic_words)

    return authoritarian_sim, control_sim, imperative_count, imperative_ratio, democratic_count, democratic_ratio

if __name__ == "__main__": 
    texts, labels, filenames = preprocess_texts("corpus")

    text = """My message is that we’ll be watching you.
2This is all wrong. I shouldn’t be up here. I should be back in school on the other side of the ocean.
Yet you all come to us young people for hope. How dare you!
3
You have stolen my dreams and my childhood with your empty words. And yet I’m one of the
lucky ones. People are suffering. People are dying. Entire ecosystems are collapsing. We are in the
beginning of a mass extinction, and all you can talk about is money and fairy tales of eternal economic growth. How dare you!
4
For more than 30 years, the science has been crystal clear. How dare you continue to look away
and come here saying that you’re doing enough, when the politics and solutions needed are still
nowhere in sight.
5You say you hear us and that you understand the urgency. But no matter how sad and angry I am,
I do not want to believe that. Because if you really understood the situation and still kept on failing
to act, then you would be evil. And that I refuse to believe.
6The popular idea of cutting our emissions in half in 10 years only gives us a 50% chance of staying
below 1.5 degrees [Celsius], and the risk of setting off irreversible chain reactions beyond human
control.
7
Fifty percent may be acceptable to you. But those numbers do not include tipping points, most
feedback loops, additional warming hidden by toxic air pollution or the aspects of equity and climate justice. They also rely on my generation sucking hundreds of billions of tons of your CO2 out
of the air with technologies that barely exist.
8So a 50% risk is simply not acceptable to us—we who have to live with the consequences.
9To have a 67% chance of staying below a 1.5 degrees global temperature rise—the best odds given by the [Intergovernmental Panel on Climate Change]—the world had 420 gigatons of CO2 left
to emit back on Jan. 1st, 2018. Today that figure is already down to less than 350 gigatons.
10How dare you pretend that this can be solved with just ‘business as usual’ and some technical
solutions? With today’s emissions levels, that remaining CO2 budget will be entirely gone within
less than 8 1/2 years.
11There will not be any solutions or plans presented in line with these figures here today, because
these numbers are too uncomfortable. And you are still not mature enough to tell it like it is.
12You are failing us. But the young people are starting to understand your betrayal. The eyes of all
future generations are upon you. And if you choose to fail us, I say: We will never forgive you.
13We will not let you get away with this. Right here, right now is where we draw the line. The world
is waking up. And change is coming, whether you like it or not."""

    vectorizer, authoritarian_mean, control_mean = feature_extraction(texts, labels, filenames)
    authoritarian_sim, control_sim, imperative_count, imperative_ratio, democratic_count, democratic_ratio = predict_similarity(
        vectorizer, authoritarian_mean, control_mean, text
    )

    """ print("\nCurrent text:")
    print(f"Authoritarian Similarity: {authoritarian_sim:.2f} | Control Similarity: {control_sim:.2f}")
    print(f"Imperative Words: {imperative_count} ({imperative_ratio:.2%}) | Democratic Words: {democratic_count} ({democratic_ratio:.2%})")
 """

    combined = combine_cos_sim_and_word_count(
    sim_auth=authoritarian_sim,
    sim_control=control_sim,
    imperative_ratio=imperative_ratio,
    democratic_ratio=democratic_ratio,
)

# combined results
print(f"Combined Authoritarian Score: {combined['combined_auth_score']:.2f} | "
      f"Combined Control Score: {combined['combined_ctrl_score']:.2f} " )



    
    
